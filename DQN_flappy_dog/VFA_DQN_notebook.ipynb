{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbaseconda8f274007aaa44da281be693806252c7a",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Function Approximation\n",
    "\n",
    "----\n",
    "\n",
    "Represent a (state/state-action) value function with a parameterized function instead of a table. Better for 'Generalization'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall: Monte-Carlo Methods\n",
    "\n",
    "----\n",
    "\n",
    "- Policy Evaluation, value function:\n",
    "$$\n",
    "V(s) = V(s) + \\frac{1}{N(s)} (G_t - V(s))\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(s, a) = Q(s, a) + \\frac{1}{N(s, a)} (G_t - Q(s, a))\n",
    "$$\n",
    "\n",
    "- Difference between $G_t$ and $V(s)$ or $Q(s, a)$:  \n",
    "$$\n",
    "\\delta = G_t - V(s)\n",
    "$$  \n",
    "  \n",
    "  \n",
    "$$\n",
    "\\delta = G_t - Q(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall: Temporal Difference Learning\n",
    "\n",
    "----\n",
    "\n",
    "- Policy Evaluation  \n",
    "  \n",
    "$$\n",
    "V(s_t) = V(s_t) + \\alpha ((r_{t+1} + \\gamma V(s_{t+1})) - V(s_t))\n",
    "$$  \n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha ((r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})) - Q(s_t, a_t))\n",
    "$$\n",
    "\n",
    "\n",
    "- TD-error:  \n",
    "  \n",
    "$$\n",
    "\\delta = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)\n",
    "$$  \n",
    "  \n",
    "$$\n",
    "\\delta = r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Value Function Approximation\n",
    "\n",
    "----\n",
    "\n",
    "- Value Function\n",
    "\n",
    "$$\n",
    "\\hat{V} (s; w) = \\sum_{j=1}^{n} x_{j} \\dot w_{j} = x(s)^{T}\\dot w\n",
    "$$\n",
    "\n",
    "- Q-Value Function\n",
    "\n",
    "$$\n",
    "\\hat{Q}(s, a; w) = \\sum_{j=1}^{n} x_{j}(s,a)\\dot w_{j} = x(s, a)^{T} \\dot w\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\delta$:  \n",
    "\n",
    "$$\n",
    "\\delta = G_t - V(s_t; w)\n",
    "$$  \n",
    "  \n",
    "$$  \n",
    "\\delta = r_{t+1} + \\gamma V(s_{t+1}; w) - V(s_t; w)\n",
    "$$  \n",
    "  \n",
    "$$\n",
    "\\delta = r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}; w) - Q(s_t, a_t; w)\n",
    "$$  \n",
    "\n",
    "- Loss function for Value function:  \n",
    "  \n",
    "$$\n",
    "J(w) = \\mathbb{E}^{\\pi} [(G_t - V (ùë†; w))^{2}]\n",
    "$$  \n",
    "  \n",
    "  \n",
    "$$\n",
    "J(w) = \\mathbb{E}^{\\pi} [(r + \\gamma V^{\\pi} (s^{\\prime}; w) - V(s; w))^{2}]\n",
    "$$\n",
    "\n",
    "  \n",
    "\n",
    "- Loss function for Q-Value function:  \n",
    "  \n",
    "$$  \n",
    "J(w) = \\mathbb{E}^{\\pi} [(G_t - Q(s_t, a_t; w))^{2}]\n",
    "$$  \n",
    "  \n",
    "$$\n",
    "J(w) = \\mathbb{E}^{\\pi} [(r + \\gamma Q(s_{t+1}, a_{t+1}; w) - Q(s_t, a_t; w))^{2}]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Approaches(Policy Improvement)\n",
    "\n",
    "----\n",
    "\n",
    "- Monte Carlo\n",
    "\n",
    "$$\n",
    "\\Delta w = \\alpha (G_t - \\hat{Q}(s, a; w)) \\nabla_{w} \\hat{Q}(s, a; w)\n",
    "\n",
    "$$\n",
    "\n",
    "- TD-learning: SARSA\n",
    "\n",
    "$$\n",
    "\\Delta w = \\alpha (r + \\gamma \\hat{Q}(s^{\\prime}, a^{\\prime}; w) - \\hat{Q}(s, a; w)) \\nabla_{w} \\hat{Q}(s, a; w)\n",
    "\n",
    "$$\n",
    "\n",
    "- TD-learning: Q-learning\n",
    "\n",
    "$$\n",
    "\\Delta w = \\alpha (r + \\gamma max_{a^{\\prime}}\\hat{Q}(s^{\\prime}, a^{\\prime}; w) - \\hat{Q}(s, a; w)) \\nabla_{w} \\hat{Q}(s, a; w)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reforcement Learning\n",
    "\n",
    "----\n",
    "\n",
    "- Using Deep Neural Network to represent Value function and Q-Value function\n",
    "\n",
    "- Monte Carlo\n",
    "\n",
    "$$\n",
    "\\Delta w = \\alpha (G_t - \\hat{Q}(s, a; w)) \\nabla_{w} \\hat{Q}(s, a; w)\n",
    "\n",
    "$$\n",
    "\n",
    "- TD-learning: SARSA\n",
    "\n",
    "$$\n",
    "\\Delta w = \\alpha (r + \\gamma \\hat{Q}(s^{\\prime}, a^{\\prime}; w) - \\hat{Q}(s, a; w)) \\nabla_{w} \\hat{Q}(s, a; w)\n",
    "\n",
    "$$\n",
    "\n",
    "- TD-learning: Q-learning\n",
    "\n",
    "$$\n",
    "\\Delta w = \\alpha (r + \\gamma max_{a^{\\prime}}\\hat{Q}(s^{\\prime}, a^{\\prime}; w) - \\hat{Q}(s, a; w)) \\nabla_{w} \\hat{Q}(s, a; w)\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues:\n",
    "\n",
    "- Correlations between samples\n",
    "\n",
    "- Non-stationary targets\n",
    "\n",
    "## Strategies:\n",
    "\n",
    "- Experience replay\n",
    "\n",
    "- Fixed Q-targets: \n",
    "\n",
    "$$\n",
    "r + \\gamma max_{a^{\\prime}}\\hat{Q}(s^{\\prime}, a^{\\prime}; w)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning Neural Network (DQN)\n",
    "\n",
    "----\n",
    "\n",
    "- Incremental Approaches\n",
    "\n",
    "$$\n",
    "\\Delta w = \\alpha (r + \\gamma max_{a^{\\prime}}\\hat{Q}(s^{\\prime}, a^{\\prime}; w^{-}) - \\hat{Q}(s, a; w)) \\nabla_{w} \\hat{Q}(s, a; w)\n",
    "$$\n",
    "\n",
    "- where $\\hat{Q}(s^{\\prime}, a^{\\prime}; w^{-})$ and $\\hat{Q}(s, a; w)$ are using different parameters.\n",
    "\n",
    "\n",
    "- Psudocode:\n",
    "  \n",
    "    1. Initialize replay memory $D$ to capacity $N$  \n",
    "    1. Initialize action-value function $Q$ with random weights $\\theta$  \n",
    "    1. Initialize target action-value function $\\hat{Q}$ with weigths $\\theta^- = \\theta$  \n",
    "    1. For episode = 1, $M$ do  \n",
    "    1. $\\quad$ Initialize sequence $s_1 = \\{x_1\\}$ and preprocessed sequence $\\phi_1=\\phi(s_1)$  \n",
    "    1. $\\quad$ For $t=1$, $T$ do  \n",
    "    1. $\\qquad$ With probability $Œµ$ select $a_t=max_a\\hat{Q}(\\phi(s_t), a; \\theta)$; otherwise select a random action $a_t$  \n",
    "    1. $\\qquad$ Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t + 1}$  \n",
    "    1. $\\qquad$ Set $s_{t+1}=s_t, a_t, x_{t+1}$ and preprocess $\\phi_{t+1}=\\phi(s_{t+1})$  \n",
    "    1. $\\qquad$ Store transition $(\\phi_t, a_t, r_t, \\phi_{t+1})$ in $D$  \n",
    "    1. $\\qquad$ Sample random minibatch of transitions $(\\phi_j, a_j, r_j, \\phi_{j+1})$ from $D$  \n",
    "    1. $\\qquad$ Set $y_j = \\begin{cases}\n",
    "    r_j& \\text{for terminal at step j+1}\\\\\n",
    "    r_j+\\gamma*max_{a'}\\hat{Q} (\\phi_{j+1}, a'; \\theta^-)&\\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "    1. $\\qquad$ Perform a gradient descent step on $(y_j-Q(\\phi_j, a_j; \\phi))^2$ with respect to the network parameters $\\theta$  \n",
    "    1. $\\qquad$ Every $C$ steps reset $\\hat{Q} = Q$  \n",
    "    1. $\\quad$ End For  \n",
    "    1. End For  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Flappy Bird(Dog)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<img src=\"./flappy_dog.png\", width=300, height=400>\n<img src=\"./flappy_bird.png\", width=300, height=400>\n<img src=\"./flappy_bird_e.png\", width=300, height=400>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"./flappy_dog.png\", width=300, height=400>\n",
    "<img src=\"./flappy_bird.png\", width=300, height=400>\n",
    "<img src=\"./flappy_bird_e.png\", width=300, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "----\n",
    "\n",
    "- Requirements\n",
    "  - torch > 1.0.0\n",
    "  - pygame >= 1.9.6\n",
    "  - numpy >= 1.18.1\n",
    "- Installation of pygame\n",
    "    ```python\n",
    "    pip install pygame\n",
    "\n",
    "    ```\n",
    "\n",
    "## Input from keyboard\n",
    "\n",
    "```python\n",
    "python FlappyDogEnv.py  # key 'a' is 'jump'\n",
    "\n",
    "```\n",
    "\n",
    "## Play by RL agent\n",
    "\n",
    "```python\n",
    "python main.py\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Welcome to my github: https://github.com/ChenDdon/Reinforcement_Learning_one_by_one "
   ]
  }
 ]
}