{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbaseconda8f274007aaa44da281be693806252c7a",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-arm Bandit\n",
    "\n",
    "Keeping balance of exploration and exploitation.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy algorithm\n",
    "\n",
    "Explore with the probability of $\\epsilon$, that is, selecting a random arm with uniform probability. Exploit with the probability of $1‚àí\\epsilon$, that is to choose the arm with the highest current average reward.\n",
    "\n",
    "We incrementally calculate the average reward of arm $Q_0(k)=0$. \n",
    "The initial average reward ùë∏, \n",
    "For $\\forall$ùíè>ùüé,\n",
    "$$\n",
    "\\begin{split}\n",
    "Q_{n}(k) &= \\frac{1}{n}((n-1)\\cdot Q_{n-1}(k)+v_n) \\\\ \n",
    "         &= Q_{n-1}(k)+\\frac{1}{n}(v_n - Q_{n-1}(k))\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$-greedy algorithm\n",
    "\n",
    "$\\textbf{Input:}$ $K$ number of arms;  \n",
    "$\\qquad \\quad$ $R$ reward function;  \n",
    "$\\qquad \\quad$ $T$ number of times to play;  \n",
    "$\\qquad \\quad$ $\\epsilon$ probability to explore.  \n",
    "01: $r=0$;  \n",
    "02: $\\forall i=1,2,...,K: Q(i)=0,count(i)=0$;  \n",
    "03: $\\textbf{For}$ $t=1,2,...,T$  \n",
    "04: $\\quad$ $\\textbf{If}$ $rand()<\\epsilon$  \n",
    "05: $\\quad$ $\\quad$ choose $k\\in\\{1,2,...K\\}$  \n",
    "06: $\\quad$ $\\textbf{Else}$  \n",
    "07: $\\quad$ $\\quad$ $k=$arg $max_i Q(i)$  \n",
    "08: $\\quad$ $\\textbf{End If}$  \n",
    "09: $\\quad$ $v=R(k)$;  \n",
    "10: $\\quad$ $r=r+v$;  \n",
    "11: $\\quad$ $Q(k)=\\frac{Q(k)\\cdot count(k)+v}{count(k)+1}$  \n",
    "12: $\\quad$ $count(k) = count(k)+1$  \n",
    "13: $\\textbf{End For}$  \n",
    "$\\textbf{Output:}$ $r$ cumulative reward\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax algorithm\n",
    "Based on the current known average reward of all the arms, SOFTMAX makes a compromise between exploration and exploitation.\n",
    "The probability distribution of each machine(arm) is based on the Boltzmann distribution:\n",
    "$$\n",
    "P(k)=\\frac{e^{\\frac{Q(k)}{\\tau}}}{\\sum^{K}_{i=1} e^{\\frac{Q(i)}{\\tau}}}\n",
    "$$\n",
    "where $Q(i)$ is current average reward of arm $i$; $\\tau > 0$ is a parameter called temperature. Temperature is lower, the arm with large value will get larger probability to be chosen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax algorithm\n",
    "\n",
    "$\\textbf{Input:}$ $K$ number of arms;  \n",
    "$\\qquad \\quad$ $R$ reward function;  \n",
    "$\\qquad \\quad$ $T$ number of times to play;  \n",
    "$\\qquad \\quad$ $\\tau$ temperature.  \n",
    "01: $r=0$;  \n",
    "02: $\\forall i=1,2,...,K: Q(i)=0,count(i)=0$;  \n",
    "03: $\\textbf{For}$ $t=1,2,...,T$  \n",
    "04: $\\quad$ $\\textbf{If}$ $rand()<\\epsilon$  \n",
    "05: $\\quad$ $\\quad$ choose $k$ from Boltzmann distribution    \n",
    "06: $\\quad$ $v=R(k)$;  \n",
    "07: $\\quad$ $r=r+v$;  \n",
    "08: $\\quad$ $Q(k)=\\frac{Q(k)\\cdot count(k)+v}{count(k)+1}$  \n",
    "09: $\\quad$ $count(k) = count(k)+1$  \n",
    "10: $\\textbf{End For}$  \n",
    "$\\textbf{Output:}$ $r$ cumulative reward\n",
    "\n",
    "-----"
   ]
  }
 ]
}
