{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based Reinforcement Learning\n",
    "\n",
    "----\n",
    "\n",
    "Rely on the model of the environment, which including reward function or/and transition model.  With the model, we can learn or infer how the environment would interact with and provide feedback to the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Algorithm Components\n",
    "\n",
    "## Model\n",
    "\n",
    "Transition: The transition function $P$ records the probability of transitioning from state $s$ to $s’$ after taking action $a$ while obtaining immediate reward $r$. \n",
    "$$\n",
    "P(s' \\vert s, a)  = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a]\n",
    "$$\n",
    "\n",
    "Reward: Reward function $R$ predicts the immediate reward triggered by one action. **Note:** Reward is sometimes defined as a function of the current state, $R(s)$, or as a function of\n",
    "the (state, action, next state) tuple, $R(s, a, s')$. Most frequently in this example, we assume reward is a function of (state, action) pair, $R(s, a)$.\n",
    "\n",
    "$$\n",
    "R(s, a) = \\mathbb{E} [R_{t+1} \\vert S_t = s, A_t = a]\n",
    "$$\n",
    "\n",
    "\n",
    "## Policy\n",
    "Policy, as the agent’s behavior function π, tells us which action to take in state s. It is a mapping from state s to action a and can be either deterministic or stochastic.\n",
    "\n",
    "  1. Deterministic policy: $$\\pi(s) = a$$\n",
    "  1. Stochastic policy: $$\\pi (a|s) = \\mathbb{P}(A=a| S=s)$$\n",
    "\n",
    "## Value\n",
    "Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of feature reward. \n",
    "\n",
    "There are many ways to define the value function. In this example we just use $\\gamma$ discount sum of reward, and the feature reward, known as **return**, is a total sum of discounted rewards going forward. We can compute the return $G_t$ starting from time $t$.\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "The discount factor properties:\n",
    "- $\\gamma \\in [0, 1]$;\n",
    "- Discounting provides mathematical convenience;\n",
    "- No need to worry about the infinite loops.\n",
    "\n",
    "\n",
    "The expected return of a particular state $s$ start from time $t$, $S_t=s$:\n",
    "$$\n",
    "V^{\\pi}(s) = \\mathbb{E}^{\\pi}[G_t \\vert S_t = s]\n",
    "$$\n",
    "\n",
    "Similarly, Q-value:\n",
    "$$\n",
    "Q^{\\pi}(s, a) = \\mathbb{E}^{\\pi}[G_t \\vert S_t = s, A_t = a]\n",
    "$$\n",
    "\n",
    "\n",
    "Additionally, using the probility distribution over possible actions and the Q-values to recover the value function, under particular policy $π$:\n",
    "$$\n",
    "V^{\\pi}(s) = \\sum_{a \\in \\mathcal{A}} Q^{\\pi}(s, a) \\pi(a \\vert s)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "Markov assumption: \n",
    "\n",
    "- The future and the past are conditionally independent given the present.\n",
    "\n",
    "A Markov deicison process consists of five elements:\n",
    "$$\n",
    "\\mathcal{M} = <S, A, P, R, \\gamma>\n",
    "$$\n",
    "\n",
    "- $S$ - state space;\n",
    "- $A$ - action space;\n",
    "- $P$ - transition function;\n",
    "- $R$ - reward function;\n",
    "- $\\gamma$ - discounting factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equations\n",
    "\n",
    "Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values.\n",
    "\n",
    "- Fallow deterministic policy:\n",
    "\n",
    "$$\n",
    "% <![CDATA[\n",
    "\\begin{aligned}\n",
    "V^{\\pi}(s) &= R(s, \\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s'|s, \\pi(s)) V^{\\pi} (s') \\\\\n",
    "Q^{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) V^{\\pi} (s')\n",
    "\\end{aligned} %]]>\n",
    "$$\n",
    "\n",
    "\n",
    "- Fallow stochastic policy: \n",
    "$$\n",
    "% <![CDATA[\n",
    "\\begin{aligned}\n",
    "V^{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s) R(s, a) + \\gamma \\sum_{s' \\in S}\\sum_{a \\in A} \\pi(a \\vert s) P(s'|s, a) V^{\\pi} (s') \\\\\n",
    "Q^{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in S} P(s'|s, a) \\sum_{a' \\in A} \\pi(a' \\vert s') Q^{\\pi} (s', a')\n",
    "\\end{aligned} %]]>\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based RL\n",
    "\n",
    "Dynamic Programming. Using deterministic policy.\n",
    "\n",
    "## Policy Evaluation\n",
    "Policy Evaluation is to compute the value $V^π$ for a given policy $π$:\n",
    "$$\n",
    "V_{t+1}(s) \n",
    "= \\mathbb{E}_\\pi [r + \\gamma V_t(s') | S_t = s]\n",
    "= \\sum_{s', r} P(s', r \\vert s, \\pi (s)) (r + \\gamma V_k(s'))\n",
    "$$\n",
    "\n",
    "## Policy Improvement\n",
    "Based on the value functions, Policy Improvement generates a better policy π′≥π by acting greedily.\n",
    "$$\n",
    "Q^\\pi(s, a) \n",
    "= \\mathbb{E} [R_{t+1} + \\gamma V^\\pi(S_{t+1}) \\vert S_t=s, A_t=a]\n",
    "= \\sum_{s', r} P(s', r \\vert s, a) (r + \\gamma V^\\pi(s'))\n",
    "$$\n",
    "\n",
    "## Policy Iteration\n",
    "\n",
    "Policy Iteration = Policy evaluation + Policy Improvement\n",
    "\n",
    "$$\n",
    "\\pi_0 \\xrightarrow[]{\\text{evaluation}} V^{\\pi_0} \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_1 \\xrightarrow[]{\\text{evaluation}} V^{\\pi_1} \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_2 \\xrightarrow[]{\\text{evaluation}} \\dots \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_* \\xrightarrow[]{\\text{evaluation}} V^*\n",
    "$$\n",
    "\n",
    "$$\n",
    "% <![CDATA[\n",
    "\\begin{aligned}\n",
    "Q^\\pi(s, \\pi'(s))\n",
    "&= Q^\\pi(s, \\arg\\max_{a \\in \\mathcal{A}} Q^\\pi(s, a)) \\\\\n",
    "&= \\max_{a \\in \\mathcal{A}} Q^\\pi(s, a) \\geq Q^\\pi(s, \\pi(s)) = V^\\pi(s)\n",
    "\\end{aligned} %]]>\n",
    "$$"
   ]
  }
 ]
}