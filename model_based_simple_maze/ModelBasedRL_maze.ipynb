{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based Reinforcement Learning\n",
    "\n",
    "----\n",
    "\n",
    "Rely on the model of the environment, which including reward function or/and transition model.  With the model, we can learn or infer how the environment would interact with and provide feedback to the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Algorithm Components\n",
    "\n",
    "----\n",
    "\n",
    "## Model\n",
    "\n",
    "- Transition: The transition function $P$ records the probability of transitioning from state $s$ to $s^{\\prime}$ after taking action $a$ while obtaining immediate reward $r$. \n",
    "$$\n",
    "P(s^{\\prime} \\vert s, a)  = \\mathbb{P} [S_{t+1} = s^{\\prime} \\vert S_t = s, A_t = a]\n",
    "$$\n",
    "\n",
    "- Reward: Reward function $R$ predicts the immediate reward triggered by one action. **Note:** Reward is sometimes defined as a function of the current state, $R(s)$, or as a function of\n",
    "the (state, action, next state) tuple, $R(s, a, s^{\\prime})$. Most frequently in this example, we assume reward is a function of (state, action) pair, $R(s, a)$.\n",
    "\n",
    "$$\n",
    "R(s, a) = \\mathbb{E} [R_{t+1} \\vert S_t = s, A_t = a]\n",
    "$$\n",
    "\n",
    "\n",
    "## Policy\n",
    "\n",
    "- Policy, as the agent’s behavior function $\\pi $, tells us which action to take in state $s$. It is a mapping from state s to action a and can be either deterministic or stochastic.\n",
    "\n",
    "  - Deterministic policy: $$\\pi (s) = a$$\n",
    "  - Stochastic policy: $$\\pi (a|s) = \\mathbb{P}(A=a| S=s)$$\n",
    "\n",
    "## Value\n",
    "\n",
    "- Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of feature reward. \n",
    "\n",
    "- There are many ways to define the value function. In this example we just use $\\gamma$ discount sum of reward, and the feature reward, known as **return**, is a total sum of discounted rewards going forward. We can compute the return $G_t$ starting from time $t$.\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "- The discount factor properties:\n",
    "  - $\\gamma \\in [0, 1]$;\n",
    "  - Discounting provides mathematical convenience;\n",
    "  - No need to worry about the infinite loops.\n",
    "\n",
    "- The expected return of a particular state $s$ start from time $t$, $S_t=s$:\n",
    "$$\n",
    "V^{\\pi}(s) = \\mathbb{E}^{\\pi}[G_t \\vert S_t = s]\n",
    "$$\n",
    "\n",
    "- Similarly, Q-value:\n",
    "$$\n",
    "Q^{\\pi}(s, a) = \\mathbb{E}^{\\pi}[G_t \\vert S_t = s, A_t = a]\n",
    "$$\n",
    "\n",
    "- Additionally, using the probility distribution over possible actions and the Q-values to recover the value function, under particular policy $π$:\n",
    "$$\n",
    "V^{\\pi}(s) = \\sum_{a \\in A} Q^{\\pi}(s, a) \\pi(a \\vert s)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "----\n",
    "\n",
    "- Markov assumption: The future and the past are conditionally independent given the present.\n",
    "\n",
    "- A Markov deicison process consists of five elements:\n",
    "$$\n",
    "\\mathcal{M} = <S, A, P, R, \\gamma>\n",
    "$$\n",
    "  - $S$ - state space;\n",
    "  - $A$ - action space;\n",
    "  - $P$ - transition function;\n",
    "  - $R$ - reward function;\n",
    "  - $\\gamma$ - discounting factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equations\n",
    "\n",
    "----\n",
    "\n",
    "- Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values.\n",
    "\n",
    "- Fallow deterministic policy:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V^{\\pi}(s) &= R(s, \\pi(s)) + \\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} P(s^{\\prime}|s, \\pi(s)) V^{\\pi} (s^{\\prime}) \\\\\n",
    "Q^{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s^{\\prime} \\in S} P(s^{\\prime}|s, a) V^{\\pi} (s^{\\prime})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "- Fallow stochastic policy: \n",
    "$$\n",
    "\\begin{aligned}\n",
    "V^{\\pi}(s) &= \\sum_{a \\in A} \\pi(a \\vert s) R(s, a) + \\gamma \\sum_{s^{\\prime} \\in S}\\sum_{a \\in A} \\pi(a \\vert s) P(s^{\\prime}|s, a) V^{\\pi} (s^{\\prime}) \\\\\n",
    "Q^{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s^{\\prime} \\in S} P(s^{\\prime}|s, a) \\sum_{a^{\\prime} \\in A} \\pi(a^{\\prime} \\vert s^{\\prime}) Q^{\\pi} (s^{\\prime}, a^{\\prime})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based RL\n",
    "\n",
    "----\n",
    "\n",
    "Dynamic Programming. Using deterministic policy.\n",
    "\n",
    "## Policy Evaluation\n",
    "\n",
    "- Policy Evaluation is to compute the value $V^{\\pi }$ for a given policy $\\pi $:  \n",
    "$$\n",
    "V_{t+1}(s) \n",
    "= \\mathbb{E}^{\\pi} [r + \\gamma V_t(s^{\\prime}) | S_t = s]\n",
    "= R(s, \\pi(s)) + \\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} P(s^{\\prime}|s, \\pi(s)) V^{\\pi} (s^{\\prime})\n",
    "$$\n",
    "\n",
    "- $PolicyEvaluation$\n",
    "  1. $\\textbf{Input:}$ MDP tuple $\\mathcal{M} = <S, A, P, R, \\gamma>$; Threshold $\\theta$; Policy $\\pi$;  \n",
    "  1. $\\forall s \\in S: V(s)=0$;  \n",
    "  1. For $t=1,2,3,...$ do  \n",
    "  1. $\\quad$ $\\forall s \\in S: V^{\\prime}(s) = R(s, \\pi (s)) + \\gamma \\sum_{s^{\\prime} \\in S} P(s^{\\prime}|s, \\pi (s)) V(s^{\\prime})$   \n",
    "  1. $\\quad$ if $max_{s \\in S} | V(s) - V^{\\prime}(s) | < \\theta $ then  \n",
    "  1. $\\quad$ $\\quad$ break  \n",
    "  1. $\\quad$ else  \n",
    "  1. $\\quad$ $\\quad$ $V=V^{\\prime}$  \n",
    "  1. $\\quad$ end if  \n",
    "  1. End for  \n",
    "  1. $\\textbf{Output:}$ Value function $V$\n",
    "\n",
    "\n",
    "## Policy Improvement\n",
    "\n",
    "- Based on the value functions, policy improvement generates a better policy ${\\pi}^{\\prime} \\geq \\pi$ by acting greedily.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^\\pi(s, a) \n",
    "= \\mathbb{E}^{\\pi} [r + \\gamma V_t(s^{\\prime}) \\vert S_t=s, A_t=a]\n",
    "= R(s, a) + \\gamma \\sum_{s^{\\prime} \\in S} P(s^{\\prime}|s, a) V^{\\pi} (s^{\\prime})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^\\pi(s, \\pi^{\\prime}(s)) = Q^\\pi(s, \\arg\\max_{a \\in A} Q^\\pi(s, a))\n",
    "$$\n",
    "\n",
    "## Policy Iteration\n",
    "\n",
    "\n",
    "- The policy iteration refers to an iterative procedure to improve the policy when combining policy evaluation and improvement.\n",
    "- Policy Iteration = Policy evaluation + Policy Improvement\n",
    "\n",
    "$$\n",
    "\\pi_{0} \\stackrel{\\text{evaluation}} {\\longrightarrow} V^{\\pi_0} \\stackrel{\\text{improve}} {\\longrightarrow} \n",
    "\\pi_{1} \\stackrel{\\text{evaluation}} {\\longrightarrow} V^{\\pi_1} \\stackrel{\\text{improve}} {\\longrightarrow} \n",
    "\\pi_{2} \\stackrel{\\text{evaluation}} {\\longrightarrow} \\dots \\stackrel{\\text{improve}} {\\longrightarrow} \n",
    "\\pi_{*} \\stackrel{\\text{evaluation}} {\\longrightarrow} V^{*}\n",
    "$$\n",
    "\n",
    "\n",
    "- This policy iteration process works and always converges to the optimality:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^{\\pi}(s, {\\pi}^{\\prime}(s))\n",
    "&= Q^{\\pi}(s, \\arg\\max_{a \\in A} Q^{\\pi}(s, a)) \\\\\n",
    "&= \\max_{a \\in A} Q^\\pi(s, a) \\\\\n",
    "&\\geq Q^\\pi(s, \\pi(s)) \\\\\n",
    "&= V^\\pi(s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $PolicyIteration$\n",
    "  1. $\\textbf{Input:}$ MDP tuple $\\mathcal{M} = <S, A, P, R, \\gamma>$; Threshold $\\theta$  \n",
    "  1. $\\forall s \\in S: V(s)=0$; Randomly Initialize $\\pi(s,a)$  \n",
    "  1. $\\textbf{Loop}$  \n",
    "  1. $\\quad$ For $t=1,2,3,...$ do  \n",
    "  1. $\\quad$ $\\quad$ $\\forall s \\in S: V^{\\prime}(s) = R(s, \\pi (s)) + \\gamma \\sum_{s^{\\prime} \\in S} P(s'|s, \\pi (s)) V(s^{\\prime})$   \n",
    "  1. $\\quad$ $\\quad$ if $max_{s \\in S} | V(s) - V'(s) | < \\theta $ then  \n",
    "  1. $\\quad$ $\\quad$ $\\quad$ break  \n",
    "  1. $\\quad$ $\\quad$ else  \n",
    "  1. $\\quad$ $\\quad$ $\\quad$ $V=V^{\\prime}$  \n",
    "  1. $\\quad$ $\\quad$ end if  \n",
    "  1. $\\quad$ End for  \n",
    "  1. $\\quad$ $\\forall s \\in S: {\\pi}^{\\prime} (s) = arg max_{a \\in A} Q(s, a)$  \n",
    "  1. $\\quad$ if $\\pi^{\\prime}(s)==\\pi(s)$ then  \n",
    "  1. $\\quad$ $\\quad$ break  \n",
    "  1. $\\quad$ else  \n",
    "  1. $\\quad$ $\\quad$ $\\pi = {\\pi}^{\\prime}$  \n",
    "  1. $\\quad$ end if  \n",
    "  1. $\\textbf{End Loop}$  \n",
    "  1. $\\textbf{Output:}$ Optimal Policy $\\pi$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: MAZE\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "- Rule: Move the red circle to the yellow square without touching any black square. The black squares are traps and the yellow square is the terminal.\n",
    "\n",
    "![MAZE demo](./Demo.png)\n",
    "\n",
    "- Given Model\n",
    "    - Transition function: $\\forall s \\in S, \\forall a \\in A, P(s^{\\prime}|s, a)=1$\n",
    "    - Reward function:\n",
    "\n",
    "| 0   | -1  | 0.2 | 0.1 | 0.1 | 0   |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: |    \n",
    "| 0.1 | -1  | 0.3 | -1  | -1  | 0   |  \n",
    "| 0.3 | 0.3 | 0.4 | 0.3 | 0.2 | 0.1 |  \n",
    "| -1  | 0.4 | 0.5 | -1  | 0   | -1  |  \n",
    "| 0   | -1  | 0.6 | -1  | 0   | 0   |  \n",
    "| 0   | 0   | 10  | -1  | 0   | -1  |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "----\n",
    "\n",
    "`cd` local folder, and run\n",
    "```python\n",
    "python main.py\n",
    "```\n"
   ]
  }
 ]
}